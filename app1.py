import streamlit as st
import pandas as pd
import numpy as np
import pickle

# --- Global Configuration ---
# Define the MINIMUM required raw columns to generate ALL features
REQUIRED_RAW_COLUMNS = ['price', 'total_revenue', 'booking_count', 'avg_stay', 'type']


# ---------------- Load Model (Cached for performance) ----------------
@st.cache_resource
def load_model():
    """Loads the pre-trained clustering model components."""
    try:
        # Assumes the pickle file is available in the working directory
        # This file must be generated by running 'train_model.py'
        with open("hotel_clustering_model.pkl", "rb") as f:
            return pickle.load(f)
    except FileNotFoundError:
        st.error("‚ùå Error: 'hotel_clustering_model.pkl' not found. Please ensure 'train_model.py' was run and the model file is in the same directory.")
        st.stop()
    except Exception as e:
        st.error(f"‚ùå Error loading model: {e}")
        st.stop()

model = load_model()

# Extract model components
iso = model.get("iso")
scaler = model.get("scaler")
kmeans = model.get("kmeans")
pca = model.get("pca")
features = model.get("features")
name_map = model.get("name_map", {}) # Extract name_map, with a fallback to empty dict

# Check for essential components
if not all([scaler, kmeans, features]):
    st.error("‚ùå Model components (scaler, kmeans, features) are missing or incomplete. Cannot proceed.")
    st.stop()

# ---------------- Page Title ----------------
st.set_page_config(layout="wide", page_title="üè® Hotel Room Clustering App")
st.title("üè® Hotel Room Clustering App")
st.markdown("---")

# ---------------- Shared Feature Generation Function ----------------
def generate_features(df):
    """
    Generates required features for clustering from raw data.
    """
    df_copy = df.copy()
    
    # 1. Check for missing raw columns (Using the global list)
    missing_raw = [col for col in REQUIRED_RAW_COLUMNS if col not in df_copy.columns]
    if missing_raw:
        st.error(
            f"‚ùå Critical Error: The CSV file is missing required base columns. "
            f"Please ensure the following columns are provided: **{', '.join(REQUIRED_RAW_COLUMNS)}**"
        )
        return None

    # 2. Feature Engineering
    try:
        # Apply Log transforms
        df_copy['log_price']     = np.log1p(df_copy['price'])
        df_copy['log_revenue']   = np.log1p(df_copy['total_revenue'])
        df_copy['log_bookings']  = np.log1p(df_copy['booking_count'])
        df_copy['log_avg_stay']  = np.log1p(df_copy['avg_stay'])
        
        # Calculate and log revenue per day
        df_copy['log_rev_per_day'] = np.log1p(df_copy['total_revenue'] / (df_copy['avg_stay'] + 1))

        # Premium indicator (handling case where 'type' might be missing in single prediction)
        if 'type' not in df_copy.columns:
             df_copy['type'] = 'Standard' # Dummy value for safe execution
             
        df_copy['is_premium_calculated'] = df_copy['type'].isin(['Suite','Deluxe','Presidential']).astype(int)

        return df_copy

    except Exception as e:
        st.error(f"‚ùå Error during feature generation. Check data types in the base columns: {e}")
        return None

# ---------------- SECTION 1: Batch Prediction (Upload CSV) ----------------
st.header("1. Batch Prediction via CSV Upload")
st.info(f"The file must contain the following minimum columns: **{', '.join(REQUIRED_RAW_COLUMNS)}**")

uploaded_file = st.file_uploader("Upload Rooms CSV File", type=["csv"])

if uploaded_file:
    # Ensure the file object pointer is at the beginning
    uploaded_file.seek(0) 
    
    try:
        df = pd.read_csv(uploaded_file)
    except Exception as e:
        st.error(f"‚ùå Failed to read CSV file. Please check file formatting or encoding: {e}")
        st.stop()

    st.write("### Uploaded Data Preview")
    st.dataframe(df.head())

    # --- Feature Generation ---
    df_processed = generate_features(df)
    
    if df_processed is not None:
        
        # Finalize 'is_premium' column
        if 'is_premium_calculated' in df_processed.columns:
            df_processed['is_premium'] = df_processed['is_premium_calculated']
            df_processed = df_processed.drop(columns=['is_premium_calculated'], errors='ignore')

        # --- Scaling and Prediction ---
        try:
            X_scaled = scaler.transform(df_processed[features])
            cluster_labels = kmeans.predict(X_scaled)
            df_processed["Cluster_ID"] = cluster_labels
            
            # Use the name map for descriptive results
            df_processed["Cluster_Name"] = df_processed["Cluster_ID"].map(name_map).fillna("Unknown")

        except Exception as e:
             st.error(f"‚ùå Error during scaling or prediction: {e}. Check for data type mismatches with the trained model.")
             st.stop()

        st.subheader("Clustered Results")
        st.dataframe(df_processed.head(10))
        
        # --- Cluster Summary ---
        st.subheader("Cluster Distribution")
        cluster_counts = df_processed['Cluster_Name'].value_counts().reset_index()
        cluster_counts.columns = ['Cluster_Name', 'Count']
        st.bar_chart(cluster_counts, x='Cluster_Name', y='Count')


        # --- Visualization ---
        if pca is not None:
            try:
                X_pca = pca.transform(X_scaled)
                df_processed["PCA1"] = X_pca[:, 0]
                df_processed["PCA2"] = X_pca[:, 1]
                
                st.write("### PCA Scatter Plot (Clustering Visualization)")
                # Convert cluster ID to string for better color mapping in chart
                df_processed['Cluster_ID_Str'] = df_processed['Cluster_ID'].astype(str)
                st.scatter_chart(df_processed, x="PCA1", y="PCA2", color="Cluster_ID_Str")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Could not generate PCA plot: {e}. Check PCA object integrity.")

        # --- Download ---
        csv = df_processed.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="Download Clustered CSV",
            data=csv,
            file_name="clustered_rooms.csv",
            mime="text/csv",
            key="download_batch"
        )

# ---------------- SECTION 2: Single Prediction Form ----------------
st.markdown("---")
st.header("2. Predict Cluster for a Single Room")
st.info("Note: The model uses the same feature transformations (e.g., Log) that it was trained on.")

# Using session state to manage single prediction result
if 'single_prediction' not in st.session_state:
    st.session_state.single_prediction = None

with st.form("single_room_form"):
    col1, col2 = st.columns(2)
    with col1:
        price = st.number_input("Price", min_value=0.0, value=150.0, step=10.0)
        revenue = st.number_input("Total Revenue", min_value=0.0, value=5000.0, step=100.0)
    with col2:
        booking_count = st.number_input("Booking Count", min_value=0, value=30, step=1)
        avg_stay = st.number_input("Average Stay Days", min_value=0.0, value=3.5, step=0.1)
    
    # Directly input the premium indicator 
    is_premium = st.selectbox("Is Premium Room? (0=No, 1=Yes)", [0, 1])

    submit = st.form_submit_button("Predict")

if submit:
    # --- Create DataFrame for Single Prediction ---
    input_data = pd.DataFrame([{
        'price': price, 
        'total_revenue': revenue, 
        'booking_count': booking_count, 
        'avg_stay': avg_stay, 
        # Add a dummy 'type' to satisfy the feature generation function's internal checks
        'type': 'Standard' 
    }])

    # --- Feature Generation for Single Input ---
    df_single = generate_features(input_data)
    
    if df_single is not None:
        # Override the calculated 'is_premium' with the user's explicit input (0 or 1)
        df_single['is_premium'] = is_premium
        df_single = df_single.drop(columns=['is_premium_calculated', 'type'], errors='ignore')

        # --- Scaling and Prediction ---
        try:
            X_scaled_single = scaler.transform(df_single[features])
            cluster_result_id = kmeans.predict(X_scaled_single)[0]
            
            # Get the descriptive name
            cluster_result_name = name_map.get(cluster_result_id, f"Cluster ID {cluster_result_id}")

            st.session_state.single_prediction = cluster_result_name
        except Exception as e:
            st.error(f"‚ùå Prediction failed: {e}. Please check the validity of numerical inputs.")

if st.session_state.single_prediction is not None:
    st.subheader("Prediction Result")
    st.success(f"The room belongs to: **{st.session_state.single_prediction}**")